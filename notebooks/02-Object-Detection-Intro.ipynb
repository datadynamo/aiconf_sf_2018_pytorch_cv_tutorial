{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Object Detection?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the location of the object along with the class is called Object Detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*cpe4z05DgTJvm0SG0MsrjA.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object Detection is modeled as a classification problem where we take windows of fixed sizes from input image at all the possible locations feed these patches to an image classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals of this Session**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding:\n",
    "\n",
    "1. Two common Datasets for object detection\n",
    "2. Object detection metrics\n",
    "3. A short history of object detectors\n",
    "\n",
    "Followed by a hands-on example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While various datasets exist for Object Detection, here we focus on the two you will encounter most in research and validation of state-of-the-art approaches. Many other datasets often serve a specific use case (e.g. annotated traffic scenes - KITTI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pascal VOC (2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name:\n",
    "- Pascal Visual Object Classes\n",
    "\n",
    "Data:\n",
    "- 2012 dataset has 11,530 images containing annotated 27,450 objects\n",
    "- 20 classes (will be covered in follow up hands on)\n",
    "- Images originally obtained from Flickr\n",
    "\n",
    "Source:\n",
    "- Pascal VOC challenges 2005-2012\n",
    "- http://host.robots.ox.ac.uk/pascal/VOC/\n",
    "- http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotation sample:\n",
    "```\n",
    "<annotation>\n",
    "\t<folder>GeneratedData_Train</folder>\n",
    "\t<filename>000001.png</filename>\n",
    "\t<path>/my/path/GeneratedData_Train/000001.png</path>\n",
    "\t<source>\n",
    "\t\t<database>Unknown</database>\n",
    "\t</source>\n",
    "\t<size>\n",
    "\t\t<width>224</width>\n",
    "\t\t<height>224</height>\n",
    "\t\t<depth>3</depth>\n",
    "\t</size>\n",
    "\t<segmented>0</segmented>\n",
    "\t<object>\n",
    "\t\t<name>21</name>\n",
    "\t\t<pose>Frontal</pose>\n",
    "\t\t<truncated>0</truncated>\n",
    "\t\t<difficult>0</difficult>\n",
    "\t\t<occluded>0</occluded>\n",
    "\t\t<bndbox>\n",
    "\t\t\t<xmin>82</xmin>\n",
    "\t\t\t<xmax>172</xmax>\n",
    "\t\t\t<ymin>88</ymin>\n",
    "\t\t\t<ymax>146</ymax>\n",
    "\t\t</bndbox>\n",
    "\t</object>\n",
    "</annotation>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look into Pascal VOC during the follow-up hands-on example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels in VOC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aeroplane',\n",
       " 'bicycle',\n",
       " 'bird',\n",
       " 'boat',\n",
       " 'bottle',\n",
       " 'bus',\n",
       " 'car',\n",
       " 'cat',\n",
       " 'chair',\n",
       " 'cow',\n",
       " 'diningtable',\n",
       " 'dog',\n",
       " 'horse',\n",
       " 'motorbike',\n",
       " 'person',\n",
       " 'pottedplant',\n",
       " 'sheep',\n",
       " 'sofa',\n",
       " 'train',\n",
       " 'tvmonitor']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow','diningtable','dog', 'horse',\n",
    " 'motorbike','person','pottedplant','sheep','sofa','train','tvmonitor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VOC Data example from paper:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/VOC_example.png' width=\"65%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name:\n",
    "- Common Objects in context\n",
    "\n",
    "Data:\n",
    "- Everyday scenes of \"common objects\"\n",
    "- Large-scale object detection, segmentation, and captioning dataset. \n",
    "- Over 200K images with 1.5M objects\n",
    "- 80 object categories\n",
    "- Additional annotations for 91 \"Stuff\" classes available\n",
    "- Stuff classes are background materials that are defined by homogeneous or repetitive patterns\n",
    "- COCO dataset reader in `torchvision.datasets.CocoDetection`\n",
    "\n",
    "Source:\n",
    "- Microsoft\n",
    "- http://cocodataset.org/\n",
    "- https://arxiv.org/abs/1405.0312"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels in COCO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person',\n",
       " 'bicycle',\n",
       " 'car',\n",
       " 'motorcycle',\n",
       " 'airplane',\n",
       " 'bus',\n",
       " 'train',\n",
       " 'truck',\n",
       " 'boat',\n",
       " 'traffic light',\n",
       " 'fire hydrant',\n",
       " 'stop sign',\n",
       " 'parking meter',\n",
       " 'bench',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'dog',\n",
       " 'horse',\n",
       " 'sheep',\n",
       " 'cow',\n",
       " 'elephant',\n",
       " 'bear',\n",
       " 'zebra',\n",
       " 'giraffe',\n",
       " 'backpack',\n",
       " 'umbrella',\n",
       " 'handbag',\n",
       " 'tie',\n",
       " 'suitcase',\n",
       " 'frisbee',\n",
       " 'skis',\n",
       " 'snowboard',\n",
       " 'sports ball',\n",
       " 'kite',\n",
       " 'baseball bat',\n",
       " 'baseball glove',\n",
       " 'skateboard',\n",
       " 'surfboard',\n",
       " 'tennis racket',\n",
       " 'bottle',\n",
       " 'wine glass',\n",
       " 'cup',\n",
       " 'fork',\n",
       " 'knife',\n",
       " 'spoon',\n",
       " 'bowl',\n",
       " 'banana',\n",
       " 'apple',\n",
       " 'sandwich',\n",
       " 'orange',\n",
       " 'broccoli',\n",
       " 'carrot',\n",
       " 'hot dog',\n",
       " 'pizza',\n",
       " 'donut',\n",
       " 'cake',\n",
       " 'chair',\n",
       " 'couch',\n",
       " 'potted plant',\n",
       " 'bed',\n",
       " 'dining table',\n",
       " 'toilet',\n",
       " 'tv',\n",
       " 'laptop',\n",
       " 'mouse',\n",
       " 'remote',\n",
       " 'keyboard',\n",
       " 'cell phone',\n",
       " 'microwave',\n",
       " 'oven',\n",
       " 'toaster',\n",
       " 'sink',\n",
       " 'refrigerator',\n",
       " 'book',\n",
       " 'clock',\n",
       " 'vase',\n",
       " 'scissors',\n",
       " 'teddy bear',\n",
       " 'hair drier',\n",
       " 'toothbrush']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', \n",
    " 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', \n",
    " 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', \n",
    " 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', \n",
    " 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', \n",
    " 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', \n",
    " 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', \n",
    " 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation in Object Detection needs to measure 2 tasks \n",
    "\n",
    "1. Object existence (classification) \n",
    "2. Object location (regression) \n",
    "\n",
    "Over a non-uniform distribution of classes at various levels of confidence. \n",
    "\n",
    "This is non-trivial. mAP was introduced as evaluation metric for object detection. In order to break down AP (Average Precision), we will first (re-)visit Precision and Recall metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "**Precision**\n",
    "  - False Positive Rate\n",
    "  - `true object detections / total number of objects predicted`\n",
    "  - How many selected items are relevant?\n",
    "\n",
    "**Recall**\n",
    "  - False Negative Rate \n",
    "  - `true object detections / total number of objects in dataset`\n",
    "  - How many relevant items are selected?\n",
    "  \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg\" width=\"35%\">\n",
    "\n",
    "**Average Precision (AP)**\n",
    "  - Precision-Recall Curve p(r) computed per class through varying confidence thresholds and determining precision and recall tuples for threshold\n",
    "  - AP computes the average value of p(r) as\n",
    "\n",
    "$$\\int_0^1 p(r)dr$$\n",
    "\n",
    "### Regression\n",
    "\n",
    "**Intersection over Union (IoU)**\n",
    "\n",
    "- AP is often as per below at a given IoU threshold\n",
    "- IoU determines the overlap between ground thruth and prediction\n",
    "- `area of intersection / area of union`\n",
    "- In the COCO challenge, 10 different IoU thresholds are considered (0.5 to 0.95 in steps of 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/rafaelpadilla/Object-Detection-Metrics/blob/master/aux_images/iou.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Average Precision (mAP)\n",
    "\n",
    "Bringing together classification and localization.\n",
    "\n",
    "- AP over all classes and/or IoU thresholds (minimum IoU)\n",
    "- Pascal VOC2007\n",
    "  - mAP over 20 classes at IoU=0.5\n",
    "- COCO2017\n",
    "  - mAP over 80 classes and all 10 IoU thresholds\n",
    "- If averaged over IoU thresholds, precise localization tends to be better rewarded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short History of Object Detectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two types of detectors exist:\n",
    "\n",
    "1. One-Shot Detectors: Object Detection and Classification Problem in one stage\n",
    "2. Two-Shot Detectors: Object Detection and Classification Problem in two stage\n",
    "\n",
    "Here we walk through existing model architectures for Object Detection, starting with two-shot methods introduced earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-CNN (2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Original application of CNN to the Object Detection problem\n",
    "- CNNs are computationally expensive\n",
    "- It is impossible to run CNNs on all windows generated on an image\n",
    "- R-CNN uses a **proposal algorithm called Selective Search reducing the number of possible bounding boxes** through features like texture, intensity, color and/or a measure of insideness\n",
    "- 2000 region proposals\n",
    "- All these selected boxes are fed to the CNN model after they have been resized to a fixed size\n",
    "- SVM is used to predict the class of each identified bounding box\n",
    "- https://arxiv.org/abs/1311.2524"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/R-CNN.png\" width=\"65%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPP-net (2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SPP = Spatial Pyramid Pooling\n",
    "- R-CNN was very slow (CNN on 2000 proposed regions takes time)\n",
    "- SPP-net: **Calculate CNN representation of image only once** and use that when calculating representation for region after selective search (pooling)\n",
    "- **Spatial Pooling instead of Max Pooling** after last Convolutional Layer = dividing region of arbitrary size into constant number of bins\n",
    "- That way, the input remains fixed size\n",
    "- Drawback: Not trivial to perform back propagation through Spatial Pooling Layer\n",
    "- Step towards more popular Fast R-CNN\n",
    "- https://arxiv.org/abs/1406.4729"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast R-CNN (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fixes the key problem in SPP-net: End-2-end training **propagating gradients through Spatial Pooling**\n",
    "- 25x faster than R-CNN\n",
    "- **Added Bounded Box Regression to the neural network training**\n",
    "- Now, network had 2 heads: Classification and bounding box regression\n",
    "- Independent training for classification and bounding box regression no longer needed\n",
    "- https://arxiv.org/abs/1504.08083"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Fast-R-CNN.png\" width=\"65%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster R-CNN (2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 10x faster than Fast R-CNN\n",
    "- **Replaces Selective Search in previous approaches with a small CNN called Region Proposal Network (RPN)**\n",
    "- Uses the idea of **Anchor Boxes** - predicting offsets instead of coordinates for fixed aspect ratio boxes\n",
    "- https://arxiv.org/abs/1506.01497"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Stage Object Detectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, all methods discussed were classification/regression handled object detection as a classification problem in 2 stages. There are a few methods which object detection as regression problem in one stage. A one-stage detector uses a fixed grid of boxes while two-stage detectors uses proposal networks to generate box proposals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **YOLO v1 (2015)**\n",
    "  - Detection as a regression problem: Take an input image and learn class probabilities\n",
    "  - YOLO divides image into grid and predicts bounding boxes on grid\n",
    "  - Confidence reflects accuracy of box\n",
    "  - It also predicts classifications core for each box\n",
    "  - Low confidence boxes are discarded\n",
    "  - CNN is run only once (YOLO is super fast and can be run in real-time)\n",
    "  - YOLO sees entire image and not just regions -> additional context can help reduce false positives\n",
    "  - Downside: 1 type of class per grid max, struggles with small objects, localization errors, low recall\n",
    "  - https://arxiv.org/abs/1506.02640\n",
    "- **YOLO v2 (2016)**\n",
    "  - Focus on improving recall and localization\n",
    "  - **Batch normalization added to YOLO** leading to 2% improvement in mAP\n",
    "  - Pre-trained on **higher resolution images** leading to 4% increase in mAP\n",
    "  - **Use Anchor Boxes** to predict bounding boxes (predicting offsets instead of coordinates for fixed aspect ratio anchor boxes) - more boxes predicted per image (decrease in mAP, improvement in recall)\n",
    "  - darknet-19 architecture (19 + 11 = 30 layers)\n",
    "  - https://arxiv.org/abs/1612.08242\n",
    "- **YOLO v3 (2018)**\n",
    "  - Little design changes\n",
    "  - \"As accurate as SSD, but three times faster\"\n",
    "  - Some speed has been traded off for boosts in accuracy in YOLO v3\n",
    "  - This is due to **changes to underlying CNN architecture** (darknet-53)\n",
    "  - 53 layer pre-trained on Imagenet + 53 more layers stacked onto network for detection (106 layers total)\n",
    "  - **Prediction at 3 scales** (generated through downsampling)\n",
    "  - https://arxiv.org/abs/1804.02767"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLOv1 architecture:\n",
    "\n",
    "<img src=\"images/YOLO_CNN.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO process:\n",
    "\n",
    "<img src=\"images/YOLO.png\" width=\"65%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Shot-Detector - SSD (2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Balance between speed and accuracy\n",
    "- Runs a convolutional network on image once to calculate feature map\n",
    "- Then **run a small 3x3 kernel on feature map to predict bounding boxes and classification probability**\n",
    "- Uses **Anchor Boxes** and learns offsets\n",
    "- Each convolutional layer operates at a different scale\n",
    "- https://arxiv.org/abs/1512.02325"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/SSD.png\" width=\"65%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RetinaNet (2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Main contribution is a new loss function called **Focal Loss** which significantly increases accuracy\n",
    "- Essentially a Feature Pyramid Network with replaced loss\n",
    "- Focal Loss lowers loss for well classified examples while putting focus on hard to classify examples\n",
    "- https://arxiv.org/abs/1708.02002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask R-CNN (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Covered in Segmentation tutorial\n",
    "- Extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition\n",
    "- https://arxiv.org/abs/1703.06870"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architectures: Speed and Accuracy comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AP for Object Detection algorithms as reported in YOLO v3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/YOLO_v3_performance.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/YOLO_v3_comparison.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview from https://zsc.github.io/megvii-pku-dl-course/slides/Lecture6(Object%20Detection).pdf:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/accuracy-speed.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overall Status**\n",
    "\n",
    "2017\n",
    "- Faster R-CNN with best accuracy numbers\n",
    "- SSD with good speed / precision tradeoff\n",
    "- YOLO for speed  \n",
    "\n",
    "2018\n",
    "- RetinaNet for accuracy\n",
    "- YOLO v3 for speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources**\n",
    "- https://cv-tricks.com/object-detection/faster-r-cnn-yolo-ssd/\n",
    "- https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/\n",
    "- https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4\n",
    "- http://cs231n.stanford.edu/slides/2018/cs231n_2018_ds06.pdf\n",
    "- https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088\n",
    "- https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b\n",
    "- https://datascience.stackexchange.com/questions/25119/how-to-calculate-map-for-detection-task-for-the-pascal-voc-challenge\n",
    "- https://medium.com/@umerfarooq_26378/from-r-cnn-to-mask-r-cnn-d6367b196cfd\n",
    "- https://mc.ai/yolo3-a-huge-improvement/\n",
    "- https://medium.com/@smallfishbigsea/notes-on-focal-loss-and-retinanet-9c614a2367c6\n",
    "- https://medium.com/@timothycarlen/understanding-the-map-evaluation-metric-for-object-detection-a07fe6962cf3\n",
    "- https://zsc.github.io/megvii-pku-dl-course/slides/Lecture6(Object%20Detection).pdf\n",
    "- https://hackernoon.com/understanding-yolo-f5a74bbc7967\n",
    "- http://christopher5106.github.io/object/detectors/2017/08/10/bounding-box-object-detectors-understanding-yolo.html\n",
    "- https://towardsdatascience.com/evolution-of-object-detection-and-localization-algorithms-e241021d8bad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
